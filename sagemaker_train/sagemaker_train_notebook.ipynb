{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sagemaker_train_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# About\n",
        "\n",
        "This notebook is an implementation of the model training code in the `sagemaker_train` directory, which contains the following folders and scripts\n",
        "\n",
        "*   `src` (directory)\n",
        "  * `__init__.py`\n",
        "  * `model.py`: entry point used to train the model; uses input arguments\n",
        "      * includes `ClassifierDataset` class that converts a `csv` or `json` file into a torch dataset, which is used as input data for the model training step\n",
        "      * `def main` is the main code used to train and evaluate the model using argparse\n",
        "      * `def preprocess_data.py` splits the training dataset into train/test (default test size is 0.2)\n",
        "  * `utils.py`: converts an object (`csv` or `json` file) into a Pandas dataframe, which is used in `model.py` as input data\n",
        "*   `data` (directory)\n",
        "  * includes training data files that can be used for the model\n",
        "  * recommend using `wiki_attacks.csv` to train a decently-performing model\n",
        "* `model` (directory): where the trained model will be saved\n",
        "* `eval_results` (directory): where the evaluation results (`json) format) will be saved"
      ],
      "metadata": {
        "id": "fmKPYNtwPYYQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Environment\n",
        "\n",
        "If running the code in notebook, be sure to install the following libraries"
      ],
      "metadata": {
        "id": "XvLIOjFiWeH6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USKZEIidPNIZ",
        "outputId": "0a3fc3d2-c6b9-422d-afca-ae495cfbe692"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 12.0 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 36.3 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 49.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 39.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 67 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.15.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch # version 1.10.0\n",
        "!pip install transformers # version 4.15.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure the working directory is set to the `src` directory"
      ],
      "metadata": {
        "id": "IZSAlmJEbCM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/sm_training/sagemaker_train/src')\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VkevA6abIDG",
        "outputId": "a2cf2d72-b32c-406b-b994-bad9e33e77b0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/sm_training/sagemaker_train/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining Training Arguments & Hyperparameters"
      ],
      "metadata": {
        "id": "W4jDhbGpZ5V7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following parameters can be specified and passed into training script:\n",
        "\n",
        "```\n",
        "    # Hyperparameters from launch_training_job.py get passed in as command line args.\n",
        "    parser.add_argument('--input_path', type=str)\n",
        "    parser.add_argument('--train_size', type=float, default=.85)\n",
        "    parser.add_argument('--adam_epsilon', type=float, default=1e-8)\n",
        "    parser.add_argument('--epochs', type=int, default=2)\n",
        "    parser.add_argument('--learning_rate', type=float, default=5e-5)\n",
        "    parser.add_argument('--weight_decay', type=float, default=0.0)\n",
        "    parser.add_argument('--max_data_rows', type=int, default=None)\n",
        "    parser.add_argument('--max_sequence_length', type=int, default=128)\n",
        "    parser.add_argument('--model_name', type=str, default='distilbert-base-uncased')\n",
        "    parser.add_argument('--train_batch_size', type=int, default=16)\n",
        "    parser.add_argument('--valid_batch_size', type=int, default=128)\n",
        "    parser.add_argument('--file_type', type=str, default='csv') # specify whether input file is csv or json (has to be one of the two)\n",
        "    parser.add_argument('--eval_dir', type=str, default='../eval_results') # set this to SM's model_dir path when using in SageMaker\n",
        "    parser.add_argument('--model_dir', type=str, default='../model') # where trained model is saved when running the script locally (outside of SageMaker)\n",
        "```\n",
        "\n",
        "Training Arguments in the `model.py` script includes additional parameters, such as `warmup_steps` (default value of 500) and `logging_steps` (default value of 10). Be sure to change these values if you want to decrease/increase the logging frequency.\n",
        "\n",
        "```\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=os.path.join(args.model_dir, \"output\"),\n",
        "        num_train_epochs=args.epochs,\n",
        "        per_device_train_batch_size=args.train_batch_size,\n",
        "        per_device_eval_batch_size=args.valid_batch_size,\n",
        "        learning_rate=args.learning_rate,\n",
        "        adam_epsilon=args.adam_epsilon,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=args.weight_decay,\n",
        "        logging_dir=os.path.join(args.model_dir, \"logs\"),\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        load_best_model_at_end=True\n",
        "    )\n",
        "```\n",
        "\n",
        "These are sample argument values\n",
        "```\n",
        "# model_name = 'distilbert-base-uncased'\n",
        "model_name = 'distilroberta-base'\n",
        "max_sequence_length = 128\n",
        "input_file = '/content/drive/MyDrive/datasets/wiki_attacks.csv'\n",
        "output_dir = 'results'\n",
        "epochs = 2\n",
        "train_batch_size = 32\n",
        "valid_batch_size = 128\n",
        "learning_rate = 5e-5\n",
        "adam_epsilon = 1e-8\n",
        "weight_decay = 0.0\n",
        "logging_dir = 'logs'\n",
        "```"
      ],
      "metadata": {
        "id": "dlT7csW0ZiMQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Begin Training Job\n",
        "\n",
        "Run the following command to start the training in notebook"
      ],
      "metadata": {
        "id": "wDCNop96a0bQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python model.py --model_name distilroberta-base --input_path /content/drive/MyDrive/sm_training/sagemaker_train/data/wiki_attacks_sample.csv --file_type csv --test_size .15 --epochs 4 --train_batch_size 16 --model_dir /content/drive/MyDrive/sm_training/sagemaker_train/distilroberta_model --eval_dir /content/drive/MyDrive/sm_training/sagemaker_train/distilroberta_model/output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejHAUokEYrjj",
        "outputId": "24c32e10-bf45-48a1-dec4-cf906304e85d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data contains 1000 rows\n",
            "{'input_ids': tensor([    0, 25194,   328,  1437, 20920,     6,     8,  2814,     7, 28274,\n",
            "            6, 27785,    38,  1034,    47,   101,     5,   317,     8,  2845,\n",
            "            7,   489,  8216,     4,  1773,    38,   192,    47,   348,   416,\n",
            "           57,  2171,   259,     6,   905,   162,    95,   492,    47,    10,\n",
            "          367,  5678,    14,    32,   460,  5616,    25,    10, 14732,  5135,\n",
            "         4704,    35,  1009,  6179,     7,  3116,    10,   372,  1566,  1009,\n",
            "         5404,  2838,     6,   714,     6,  2883,     6,     8,  3184, 35950,\n",
            "         1009,  2264, 28274,    16,    45,  1009, 45445, 45836,  1009, 47681,\n",
            "           18, 26266, 13565,  1986,  1009, 42124,  4583,    38,  1034,    47,\n",
            "         2254,  5390,   259,     8,   145,    10, 40823, 34740,   811,     4,\n",
            "          318,    47,    33,   143,  1142,     6,   192,     5,   244,  6052,\n",
            "            6,  1606,    10,   864,    23,     5,  3375,  9296,    50,   619,\n",
            "          481,     7,  1394,   162,    15,   127,   128,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor(0)}\n",
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "***** Running training *****\n",
            "  Num examples = 850\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 216\n",
            "{'loss': 0.5721, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.19}\n",
            "  5% 10/216 [00:03<01:13,  2.78it/s]***** Running Evaluation *****\n",
            "  Num examples = 150\n",
            "  Batch size = 128\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "100% 2/2 [00:00<00:00, 11.57it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.5681350827217102, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1_score': 0.0, 'eval_roc_auc': 0.5, 'eval_runtime': 1.0892, 'eval_samples_per_second': 137.71, 'eval_steps_per_second': 1.836, 'epoch': 0.19}\n",
            "  5% 10/216 [00:04<01:13,  2.78it/s]\n",
            "100% 2/2 [00:00<00:00, 11.57it/s]\u001b[A\n",
            "{'loss': 0.5567, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.37}\n",
            "  9% 20/216 [00:08<01:12,  2.70it/s]***** Running Evaluation *****\n",
            "  Num examples = 150\n",
            "  Batch size = 128\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "100% 2/2 [00:00<00:00, 11.62it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.5421348810195923, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1_score': 0.0, 'eval_roc_auc': 0.5, 'eval_runtime': 1.0875, 'eval_samples_per_second': 137.934, 'eval_steps_per_second': 1.839, 'epoch': 0.37}\n",
            "  9% 20/216 [00:09<01:12,  2.70it/s]\n",
            "100% 2/2 [00:00<00:00, 11.62it/s]\u001b[A\n",
            "{'loss': 0.5377, 'learning_rate': 3e-06, 'epoch': 0.56}\n",
            " 14% 30/216 [00:13<01:09,  2.68it/s]***** Running Evaluation *****\n",
            "  Num examples = 150\n",
            "  Batch size = 128\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "100% 2/2 [00:00<00:00, 11.60it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.4977598190307617, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1_score': 0.0, 'eval_roc_auc': 0.5, 'eval_runtime': 1.0895, 'eval_samples_per_second': 137.673, 'eval_steps_per_second': 1.836, 'epoch': 0.56}\n",
            " 14% 30/216 [00:14<01:09,  2.68it/s]\n",
            "100% 2/2 [00:00<00:00, 11.60it/s]\u001b[A\n",
            "{'loss': 0.4642, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.74}\n",
            " 19% 40/216 [00:17<01:06,  2.66it/s]***** Running Evaluation *****\n",
            "  Num examples = 150\n",
            "  Batch size = 128\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "100% 2/2 [00:00<00:00, 11.63it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.43004852533340454, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1_score': 0.0, 'eval_roc_auc': 0.5, 'eval_runtime': 1.0883, 'eval_samples_per_second': 137.824, 'eval_steps_per_second': 1.838, 'epoch': 0.74}\n",
            " 19% 40/216 [00:18<01:06,  2.66it/s]\n",
            "100% 2/2 [00:00<00:00, 11.63it/s]\u001b[A\n",
            "{'loss': 0.4234, 'learning_rate': 5e-06, 'epoch': 0.93}\n",
            " 23% 50/216 [00:22<01:02,  2.65it/s]***** Running Evaluation *****\n",
            "  Num examples = 150\n",
            "  Batch size = 128\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "100% 2/2 [00:00<00:00, 11.48it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.32301586866378784, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1_score': 0.0, 'eval_roc_auc': 0.5, 'eval_runtime': 1.0893, 'eval_samples_per_second': 137.709, 'eval_steps_per_second': 1.836, 'epoch': 0.93}\n",
            " 23% 50/216 [00:23<01:02,  2.65it/s]\n",
            "100% 2/2 [00:00<00:00, 11.48it/s]\u001b[A\n",
            "{'loss': 0.2629, 'learning_rate': 6e-06, 'epoch': 1.11}\n",
            " 28% 60/216 [00:26<00:56,  2.75it/s]***** Running Evaluation *****\n",
            "  Num examples = 150\n",
            "  Batch size = 128\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "100% 2/2 [00:00<00:00, 11.39it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.22170862555503845, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1_score': 0.0, 'eval_roc_auc': 0.5, 'eval_runtime': 1.0921, 'eval_samples_per_second': 137.349, 'eval_steps_per_second': 1.831, 'epoch': 1.11}\n",
            " 28% 60/216 [00:28<00:56,  2.75it/s]\n",
            "100% 2/2 [00:00<00:00, 11.39it/s]\u001b[A\n",
            "{'loss': 0.2563, 'learning_rate': 7.000000000000001e-06, 'epoch': 1.3}\n",
            " 32% 70/216 [00:31<00:54,  2.66it/s]***** Running Evaluation *****\n",
            "  Num examples = 150\n",
            "  Batch size = 128\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "100% 2/2 [00:00<00:00, 11.22it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.2125028520822525, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1_score': 0.0, 'eval_roc_auc': 0.5, 'eval_runtime': 1.1014, 'eval_samples_per_second': 136.186, 'eval_steps_per_second': 1.816, 'epoch': 1.3}\n",
            " 32% 70/216 [00:32<00:54,  2.66it/s]\n",
            "100% 2/2 [00:00<00:00, 11.22it/s]\u001b[A\n",
            "{'loss': 0.1666, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.48}\n",
            " 37% 80/216 [00:36<00:51,  2.63it/s]***** Running Evaluation *****\n",
            "  Num examples = 150\n",
            "  Batch size = 128\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "100% 2/2 [00:00<00:00, 11.40it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.2108713835477829, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1_score': 0.0, 'eval_roc_auc': 0.5, 'eval_runtime': 1.1015, 'eval_samples_per_second': 136.175, 'eval_steps_per_second': 1.816, 'epoch': 1.48}\n",
            " 37% 80/216 [00:37<00:51,  2.63it/s]\n",
            "100% 2/2 [00:00<00:00, 11.40it/s]\u001b[A\n",
            "{'loss': 0.2768, 'learning_rate': 9e-06, 'epoch': 1.67}\n",
            " 42% 90/216 [00:41<00:47,  2.65it/s]***** Running Evaluation *****\n",
            "  Num examples = 150\n",
            "  Batch size = 128\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "100% 2/2 [00:00<00:00, 11.54it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.18973353505134583, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1_score': 0.0, 'eval_roc_auc': 0.5, 'eval_runtime': 1.088, 'eval_samples_per_second': 137.87, 'eval_steps_per_second': 1.838, 'epoch': 1.67}\n",
            " 42% 90/216 [00:42<00:47,  2.65it/s]\n",
            "100% 2/2 [00:00<00:00, 11.54it/s]\u001b[A\n",
            "{'loss': 0.1826, 'learning_rate': 1e-05, 'epoch': 1.85}\n",
            " 46% 100/216 [00:45<00:44,  2.63it/s]***** Running Evaluation *****\n",
            "  Num examples = 150\n",
            "  Batch size = 128\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "100% 2/2 [00:00<00:00, 11.15it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.17752589285373688, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1_score': 0.0, 'eval_roc_auc': 0.5, 'eval_runtime': 1.1066, 'eval_samples_per_second': 135.544, 'eval_steps_per_second': 1.807, 'epoch': 1.85}\n",
            " 46% 100/216 [00:47<00:44,  2.63it/s]\n",
            "100% 2/2 [00:00<00:00, 11.15it/s]\u001b[A\n",
            "{'loss': 0.1793, 'learning_rate': 1.1000000000000001e-05, 'epoch': 2.04}\n",
            " 51% 110/216 [00:50<00:36,  2.94it/s]***** Running Evaluation *****\n",
            "  Num examples = 150\n",
            "  Batch size = 128\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "100% 2/2 [00:00<00:00, 11.12it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.1680837720632553, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1_score': 0.0, 'eval_roc_auc': 0.5, 'eval_runtime': 1.1086, 'eval_samples_per_second': 135.308, 'eval_steps_per_second': 1.804, 'epoch': 2.04}\n",
            " 51% 110/216 [00:51<00:36,  2.94it/s]\n",
            "100% 2/2 [00:00<00:00, 11.12it/s]\u001b[A\n",
            "{'loss': 0.156, 'learning_rate': 1.2e-05, 'epoch': 2.22}\n",
            " 56% 120/216 [00:55<00:36,  2.63it/s]***** Running Evaluation *****\n",
            "  Num examples = 150\n",
            "  Batch size = 128\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "100% 2/2 [00:00<00:00, 11.30it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.14030638337135315, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1_score': 0.0, 'eval_roc_auc': 0.5, 'eval_runtime': 1.1082, 'eval_samples_per_second': 135.359, 'eval_steps_per_second': 1.805, 'epoch': 2.22}\n",
            " 56% 120/216 [00:56<00:36,  2.63it/s]\n",
            "100% 2/2 [00:00<00:00, 11.30it/s]\u001b[A\n",
            "{'loss': 0.1261, 'learning_rate': 1.3000000000000001e-05, 'epoch': 2.41}\n",
            " 60% 130/216 [01:00<00:32,  2.61it/s]***** Running Evaluation *****\n",
            "  Num examples = 150\n",
            "  Batch size = 128\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "100% 2/2 [00:00<00:00, 11.66it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.2305145263671875, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1_score': 0.0, 'eval_roc_auc': 0.5, 'eval_runtime': 1.1013, 'eval_samples_per_second': 136.198, 'eval_steps_per_second': 1.816, 'epoch': 2.41}\n",
            " 60% 130/216 [01:01<00:32,  2.61it/s]\n",
            "100% 2/2 [00:00<00:00, 11.66it/s]\u001b[A\n",
            "{'loss': 0.1653, 'learning_rate': 1.4000000000000001e-05, 'epoch': 2.59}\n",
            " 65% 140/216 [01:04<00:28,  2.63it/s]***** Running Evaluation *****\n",
            "  Num examples = 150\n",
            "  Batch size = 128\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.11987075954675674, 'eval_precision': 0.625, 'eval_recall': 0.5555555555555556, 'eval_f1_score': 0.5882352941176471, 'eval_roc_auc': 0.7671394799054373, 'eval_runtime': 1.1051, 'eval_samples_per_second': 135.736, 'eval_steps_per_second': 1.81, 'epoch': 2.59}\n",
            " 65% 140/216 [01:05<00:28,  2.63it/s]\n",
            "100% 2/2 [00:00<00:00, 11.45it/s]\u001b[A\n",
            "{'loss': 0.0753, 'learning_rate': 1.5e-05, 'epoch': 2.78}\n",
            " 69% 150/216 [01:09<00:25,  2.63it/s]***** Running Evaluation *****\n",
            "  Num examples = 150\n",
            "  Batch size = 128\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.18371763825416565, 'eval_precision': 0.75, 'eval_recall': 0.3333333333333333, 'eval_f1_score': 0.46153846153846156, 'eval_roc_auc': 0.6631205673758864, 'eval_runtime': 1.1129, 'eval_samples_per_second': 134.783, 'eval_steps_per_second': 1.797, 'epoch': 2.78}\n",
            " 69% 150/216 [01:10<00:25,  2.63it/s]\n",
            "100% 2/2 [00:00<00:00, 11.32it/s]\u001b[A\n",
            "{'loss': 0.1108, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.96}\n",
            " 74% 160/216 [01:14<00:21,  2.62it/s]***** Running Evaluation *****\n",
            "  Num examples = 150\n",
            "  Batch size = 128\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.2376941740512848, 'eval_precision': 0.75, 'eval_recall': 0.3333333333333333, 'eval_f1_score': 0.46153846153846156, 'eval_roc_auc': 0.6631205673758864, 'eval_runtime': 1.1294, 'eval_samples_per_second': 132.816, 'eval_steps_per_second': 1.771, 'epoch': 2.96}\n",
            " 74% 160/216 [01:15<00:21,  2.62it/s]\n",
            "100% 2/2 [00:00<00:00, 11.20it/s]\u001b[A\n",
            "{'loss': 0.057, 'learning_rate': 1.7000000000000003e-05, 'epoch': 3.15}\n",
            " 79% 170/216 [01:19<00:17,  2.62it/s]***** Running Evaluation *****\n",
            "  Num examples = 150\n",
            "  Batch size = 128\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.2513086795806885, 'eval_precision': 0.3076923076923077, 'eval_recall': 0.4444444444444444, 'eval_f1_score': 0.3636363636363637, 'eval_roc_auc': 0.6903073286052009, 'eval_runtime': 1.1272, 'eval_samples_per_second': 133.076, 'eval_steps_per_second': 1.774, 'epoch': 3.15}\n",
            " 79% 170/216 [01:20<00:17,  2.62it/s]\n",
            "100% 2/2 [00:00<00:00, 11.37it/s]\u001b[A\n",
            "{'loss': 0.0908, 'learning_rate': 1.8e-05, 'epoch': 3.33}\n",
            " 83% 180/216 [01:23<00:13,  2.60it/s]***** Running Evaluation *****\n",
            "  Num examples = 150\n",
            "  Batch size = 128\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.3130551278591156, 'eval_precision': 0.6666666666666666, 'eval_recall': 0.2222222222222222, 'eval_f1_score': 0.3333333333333333, 'eval_roc_auc': 0.607565011820331, 'eval_runtime': 1.1183, 'eval_samples_per_second': 134.133, 'eval_steps_per_second': 1.788, 'epoch': 3.33}\n",
            " 83% 180/216 [01:25<00:13,  2.60it/s]\n",
            "100% 2/2 [00:00<00:00, 11.26it/s]\u001b[A\n",
            "{'loss': 0.0948, 'learning_rate': 1.9e-05, 'epoch': 3.52}\n",
            " 88% 190/216 [01:28<00:10,  2.59it/s]***** Running Evaluation *****\n",
            "  Num examples = 150\n",
            "  Batch size = 128\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.2356429547071457, 'eval_precision': 0.6, 'eval_recall': 0.3333333333333333, 'eval_f1_score': 0.42857142857142855, 'eval_roc_auc': 0.6595744680851063, 'eval_runtime': 1.1335, 'eval_samples_per_second': 132.328, 'eval_steps_per_second': 1.764, 'epoch': 3.52}\n",
            " 88% 190/216 [01:29<00:10,  2.59it/s]\n",
            "100% 2/2 [00:00<00:00, 11.33it/s]\u001b[A\n",
            "{'loss': 0.1327, 'learning_rate': 2e-05, 'epoch': 3.7}\n",
            " 93% 200/216 [01:33<00:06,  2.58it/s]***** Running Evaluation *****\n",
            "  Num examples = 150\n",
            "  Batch size = 128\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.2216159552335739, 'eval_precision': 0.5, 'eval_recall': 0.4444444444444444, 'eval_f1_score': 0.47058823529411764, 'eval_roc_auc': 0.7080378250591016, 'eval_runtime': 1.1262, 'eval_samples_per_second': 133.191, 'eval_steps_per_second': 1.776, 'epoch': 3.7}\n",
            " 93% 200/216 [01:34<00:06,  2.58it/s]\n",
            "100% 2/2 [00:00<00:00, 11.01it/s]\u001b[A\n",
            "{'loss': 0.089, 'learning_rate': 2.1e-05, 'epoch': 3.89}\n",
            " 97% 210/216 [01:38<00:02,  2.59it/s]***** Running Evaluation *****\n",
            "  Num examples = 150\n",
            "  Batch size = 128\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.2487887591123581, 'eval_precision': 0.75, 'eval_recall': 0.3333333333333333, 'eval_f1_score': 0.46153846153846156, 'eval_roc_auc': 0.6631205673758864, 'eval_runtime': 1.1197, 'eval_samples_per_second': 133.962, 'eval_steps_per_second': 1.786, 'epoch': 3.89}\n",
            " 97% 210/216 [01:39<00:02,  2.59it/s]\n",
            "100% 2/2 [00:00<00:00, 11.21it/s]\u001b[A\n",
            "100% 216/216 [01:41<00:00,  2.83it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 101.6651, 'train_samples_per_second': 33.443, 'train_steps_per_second': 2.125, 'train_loss': 0.23464519226992572, 'epoch': 4.0}\n",
            "100% 216/216 [01:41<00:00,  2.13it/s]\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 150\n",
            "  Batch size = 128\n",
            "100% 2/2 [00:00<00:00, 10.78it/s]\n",
            "****** Eval Results ******\n",
            "Saving model checkpoint to /content/drive/MyDrive/sm_training/sagemaker_train/distilroberta_model\n",
            "Configuration saved in /content/drive/MyDrive/sm_training/sagemaker_train/distilroberta_model/config.json\n",
            "Model weights saved in /content/drive/MyDrive/sm_training/sagemaker_train/distilroberta_model/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference Code (in progress)\n",
        "\n",
        "Resources:\n",
        "* https://github.com/aws-samples/amazon-sagemaker-bert-pytorch/blob/master/code/deploy_ei.py\n",
        "* https://github.com/aws/sagemaker-pytorch-inference-toolkit/blob/master/src/sagemaker_pytorch_serving_container/default_pytorch_inference_handler.py\n",
        "* https://github.com/aws/sagemaker-inference-toolkit/blob/master/src/sagemaker_inference/default_inference_handler.py"
      ],
      "metadata": {
        "id": "DWJr-mZXj7JH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import dependencies\n",
        "\n",
        "Note: working directory should still be the `src` directory"
      ],
      "metadata": {
        "id": "rTJEXwvwkpLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import json\n",
        "\n",
        "# import boto3\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import precision_score, recall_score, average_precision_score, roc_auc_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
        "from transformers import AdamW, AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "\n",
        "from utils import read_object"
      ],
      "metadata": {
        "id": "kdKS75kZkrTx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set variables for inference code"
      ],
      "metadata": {
        "id": "gQTMCsgtj_bP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = '/content/drive/MyDrive/sm_training/sagemaker_train/distilroberta_model'"
      ],
      "metadata": {
        "id": "WydVZ8iTj6NP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_fn(model_dir):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"================ objects in model_dir ===================\")\n",
        "    print(os.listdir(model_dir))\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "    print(\"================ model loaded ===========================\")\n",
        "    return model.to(device)"
      ],
      "metadata": {
        "id": "WiDaVxyjbqXu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fn(model_dir)"
      ],
      "metadata": {
        "id": "1lMzy1QqkW3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def input_fn(request_body, request_content_type):\n",
        "    \"\"\"An input_fn that loads a pickled tensor\"\"\"\n",
        "    if request_content_type == \"application/json\":\n",
        "        data = json.loads(request_body)\n",
        "        print(\"================ input text ===============\")\n",
        "        print(data)\n",
        "        \n",
        "        if isinstance(data, str):\n",
        "            data = [data]\n",
        "        elif isinstance(data, list) and len(data) > 0 and isinstance(data[0], str):\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported input type. Input type can be a string or an non-empty list. \\\n",
        "                             I got {}\".format(data))\n",
        "                       \n",
        "        #encoded = [tokenizer.encode(x, add_special_tokens=True) for x in data]\n",
        "        #encoded = tokenizer(data, add_special_tokens=True) \n",
        "        \n",
        "        # for backward compatibility use the following way to encode \n",
        "        # https://github.com/huggingface/transformers/issues/5580\n",
        "        input_ids = [tokenizer.encode(x, add_special_tokens=True) for x in data]\n",
        "        \n",
        "        print(\"================ encoded sentences ==============\")\n",
        "        print(input_ids)\n",
        "\n",
        "        # pad shorter sentence\n",
        "        padded =  torch.zeros(len(input_ids), MAX_LEN) \n",
        "        for i, p in enumerate(input_ids):\n",
        "            padded[i, :len(p)] = torch.tensor(p)\n",
        "     \n",
        "        # create mask\n",
        "        mask = (padded != 0)\n",
        "        \n",
        "        print(\"================= padded input and attention mask ================\")\n",
        "        print(padded, '\\n', mask)\n",
        "\n",
        "        return padded.long(), mask.long()\n",
        "    raise ValueError(\"Unsupported content type: {}\".format(request_content_type))\n",
        "    \n",
        "\n",
        "def predict_fn(input_data, model):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    input_id, input_mask = input_data\n",
        "    input_id = input_id.to(device)\n",
        "    input_mask = input_mask.to(device)\n",
        "    print(\"============== encoded data =================\")\n",
        "    print(input_id, input_mask)\n",
        "    with torch.no_grad():\n",
        "        y = model(input_id, attention_mask=input_mask)[0]\n",
        "        print(\"=============== inference result =================\")\n",
        "        print(y)\n",
        "    return y"
      ],
      "metadata": {
        "id": "_5rdU4s-kUQw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}