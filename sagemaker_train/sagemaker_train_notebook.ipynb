{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sagemaker_train_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Di7Ts4Wx0Uaw13eKY4a1lETz76rnOt6A",
      "authorship_tag": "ABX9TyPvsS37iwDkL+LUhDuTtQTX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfrizzle/NLP-Projects/blob/master/sagemaker_train/sagemaker_train_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About\n",
        "\n",
        "This notebook is an implementation of the model training code in the `sagemaker_train` directory, which contains the following folders and scripts\n",
        "\n",
        "*   `src` (directory)\n",
        "  * `__init__.py`\n",
        "  * `model.py`: entry point used to train the model; uses input arguments\n",
        "      * includes `ClassifierDataset` class that converts a `csv` or `json` file into a torch dataset, which is used as input data for the model training step\n",
        "      * `def main` is the main code used to train and evaluate the model using argparse\n",
        "      * `def preprocess_data.py` splits the training dataset into train/test (default test size is 0.2)\n",
        "  * `utils.py`: converts an object (`csv` or `json` file) into a Pandas dataframe, which is used in `model.py` as input data\n",
        "*   `data` (directory)\n",
        "  * includes training data files that can be used for the model\n",
        "  * recommend using `wiki_attacks.csv` to train a decently-performing model\n",
        "* `model` (directory): where the trained model will be saved\n",
        "* `eval_results` (directory): where the evaluation results (`json) format) will be saved"
      ],
      "metadata": {
        "id": "fmKPYNtwPYYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSJ423IdbGdt",
        "outputId": "a5690116-ead1-4b15-b19a-0ca72dbd5148"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Environment\n",
        "\n",
        "If running the code in notebook, be sure to install the following libraries"
      ],
      "metadata": {
        "id": "XvLIOjFiWeH6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USKZEIidPNIZ",
        "outputId": "3c96aafc-3e08-4ab7-859f-3cd0b25d7a5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch # version 1.10.0\n",
        "!pip install transformers # version 4.15.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure the working directory is set to the `src` directory"
      ],
      "metadata": {
        "id": "IZSAlmJEbCM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/sm_training/sagemaker_train/src')\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VkevA6abIDG",
        "outputId": "18342e20-13d3-4591-987a-6e849d80dd87"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/sm_training/sagemaker_train/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining Training Arguments & Hyperparameters"
      ],
      "metadata": {
        "id": "W4jDhbGpZ5V7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following parameters can be specified and passed into training script:\n",
        "\n",
        "```\n",
        "    # Hyperparameters from launch_training_job.py get passed in as command line args.\n",
        "    parser.add_argument('--input_path', type=str)\n",
        "    parser.add_argument('--train_size', type=float, default=.85)\n",
        "    parser.add_argument('--adam_epsilon', type=float, default=1e-8)\n",
        "    parser.add_argument('--epochs', type=int, default=2)\n",
        "    parser.add_argument('--learning_rate', type=float, default=5e-5)\n",
        "    parser.add_argument('--weight_decay', type=float, default=0.0)\n",
        "    parser.add_argument('--max_data_rows', type=int, default=None)\n",
        "    parser.add_argument('--max_sequence_length', type=int, default=128)\n",
        "    parser.add_argument('--model_name', type=str, default='distilbert-base-uncased')\n",
        "    parser.add_argument('--train_batch_size', type=int, default=16)\n",
        "    parser.add_argument('--valid_batch_size', type=int, default=128)\n",
        "    parser.add_argument('--file_type', type=str, default='csv') # specify whether input file is csv or json (has to be one of the two)\n",
        "    parser.add_argument('--eval_dir', type=str, default='../eval_results') # set this to SM's model_dir path when using in SageMaker\n",
        "    parser.add_argument('--model_dir', type=str, default='../model') # where trained model is saved when running the script locally (outside of SageMaker)\n",
        "```\n",
        "\n",
        "Training Arguments in the `model.py` script includes additional parameters, such as `warmup_steps` (default value of 500) and `logging_steps` (default value of 10). Be sure to change these values if you want to decrease/increase the logging frequency.\n",
        "\n",
        "```\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=os.path.join(args.model_dir, \"output\"),\n",
        "        num_train_epochs=args.epochs,\n",
        "        per_device_train_batch_size=args.train_batch_size,\n",
        "        per_device_eval_batch_size=args.valid_batch_size,\n",
        "        learning_rate=args.learning_rate,\n",
        "        adam_epsilon=args.adam_epsilon,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=args.weight_decay,\n",
        "        logging_dir=os.path.join(args.model_dir, \"logs\"),\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        load_best_model_at_end=True\n",
        "    )\n",
        "```\n",
        "\n",
        "These are sample argument values\n",
        "```\n",
        "# model_name = 'distilbert-base-uncased'\n",
        "model_name = 'distilroberta-base'\n",
        "max_sequence_length = 128\n",
        "input_file = '/content/drive/MyDrive/datasets/wiki_attacks.csv'\n",
        "output_dir = 'results'\n",
        "epochs = 2\n",
        "train_batch_size = 32\n",
        "valid_batch_size = 128\n",
        "learning_rate = 5e-5\n",
        "adam_epsilon = 1e-8\n",
        "weight_decay = 0.0\n",
        "logging_dir = 'logs'\n",
        "```"
      ],
      "metadata": {
        "id": "dlT7csW0ZiMQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Begin Training Job\n",
        "\n",
        "Run the following command to start the training in notebook"
      ],
      "metadata": {
        "id": "wDCNop96a0bQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python trainer.py --model_name distilroberta-base --input_path /content/drive/MyDrive/sm_training/sagemaker_train/data/wiki_attacks_sample.csv --file_type csv --test_size .20 --epochs 45 --train_batch_size 16 --model_dir /content/drive/MyDrive/sm_training/sagemaker_train/test_model --warmup_steps 100 --logging_steps 10 --eval_dir /content/drive/MyDrive/sm_training/sagemaker_train/test_model/evaluation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fem74jWmbuER",
        "outputId": "037c3c58-5f38-4600-baa4-eea9b636d757"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-01-09 06:54:20,433 - __main__ - INFO -  Dataset contains 1000 rows\n",
            "Data contains 1000 rows\n",
            "2022-01-09 06:54:23,930 - __main__ - INFO -  loaded train_dataset length is: 800\n",
            "2022-01-09 06:54:23,930 - __main__ - INFO -  loaded test_dataset length is: 200\n",
            "{'input_ids': tensor([    0,  1437,  1437, 45994, 32588, 43292,    42,   177,  1326,    98,\n",
            "        21905,   179,  4045,   939,  1034,   127,  4252, 13079,    24,    13,\n",
            "          162,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(0)}\n",
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "2022-01-09 06:54:30,229 - __main__ - INFO - ***** continue training *****\n",
            "Loading model from /content/drive/MyDrive/sm_training/sagemaker_train/test_model/checkpoint-2000).\n",
            "***** Running training *****\n",
            "  Num examples = 800\n",
            "  Num Epochs = 45\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 2250\n",
            "  Continuing training from checkpoint, will skip to saved global_step\n",
            "  Continuing training from epoch 40\n",
            "  Continuing training from global step 2000\n",
            "  Will skip the first 40 epochs then the first 0 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n",
            "Skipping the first batches: : 0it [00:00, ?it/s]\n",
            "Skipping the first batches: : 0it [00:00, ?it/s]\n",
            "\n",
            " 89% 2001/2250 [00:00<00:00, 2393.51it/s]\u001b[A\n",
            "\u001b[A{'loss': 0.0001, 'learning_rate': 5.581395348837209e-06, 'epoch': 40.2}\n",
            "\n",
            " 89% 2010/2250 [00:04<00:00, 2393.51it/s]\u001b[A***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 128\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.961802965728566e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1_score': 1.0, 'eval_roc_auc': 1.0, 'eval_runtime': 1.4445, 'eval_samples_per_second': 138.453, 'eval_steps_per_second': 1.385, 'epoch': 40.2}\n",
            "100% 2/2 [00:00<00:00,  3.84it/s]\n",
            "\n",
            "\u001b[A{'loss': 0.0001, 'learning_rate': 5.348837209302326e-06, 'epoch': 40.4}\n",
            "\n",
            " 90% 2020/2250 [00:09<00:00, 2393.51it/s]\u001b[A***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 128\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.9383199944277294e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1_score': 1.0, 'eval_roc_auc': 1.0, 'eval_runtime': 1.4363, 'eval_samples_per_second': 139.246, 'eval_steps_per_second': 1.392, 'epoch': 40.4}\n",
            "100% 2/2 [00:00<00:00,  3.86it/s]\n",
            "\n",
            "\u001b[A{'loss': 0.0, 'learning_rate': 5.116279069767442e-06, 'epoch': 40.6}\n",
            "\n",
            " 90% 2030/2250 [00:14<00:00, 2393.51it/s]\u001b[A***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 128\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.917399615398608e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1_score': 1.0, 'eval_roc_auc': 1.0, 'eval_runtime': 1.4494, 'eval_samples_per_second': 137.989, 'eval_steps_per_second': 1.38, 'epoch': 40.6}\n",
            "100% 2/2 [00:00<00:00,  3.82it/s]\n",
            "\n",
            "\u001b[A{'loss': 0.0001, 'learning_rate': 4.883720930232559e-06, 'epoch': 40.8}\n",
            "\n",
            " 91% 2040/2250 [00:19<00:00, 2393.51it/s]\u001b[A***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 128\n",
            "\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.8945723897777498e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1_score': 1.0, 'eval_roc_auc': 1.0, 'eval_runtime': 1.4456, 'eval_samples_per_second': 138.354, 'eval_steps_per_second': 1.384, 'epoch': 40.8}\n",
            "100% 2/2 [00:00<00:00,  3.84it/s]\n",
            "\n",
            " 91% 2041/2250 [00:21<00:03, 67.77it/s]  \u001b[A\n",
            " 91% 2042/2250 [00:21<00:03, 66.20it/s]\u001b[A\n",
            "\u001b[A{'loss': 0.0001, 'learning_rate': 4.651162790697674e-06, 'epoch': 41.0}\n",
            "\n",
            " 91% 2050/2250 [00:24<00:03, 66.20it/s]\u001b[A***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 128\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.8710299375234172e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1_score': 1.0, 'eval_roc_auc': 1.0, 'eval_runtime': 1.4439, 'eval_samples_per_second': 138.51, 'eval_steps_per_second': 1.385, 'epoch': 41.0}\n",
            "100% 2/2 [00:00<00:00,  3.82it/s]\n",
            "\n",
            "\u001b[A{'loss': 0.0, 'learning_rate': 4.418604651162791e-06, 'epoch': 41.2}\n",
            "\n",
            " 92% 2060/2250 [00:30<00:02, 66.20it/s]\u001b[A***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 128\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.851242243195884e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1_score': 1.0, 'eval_roc_auc': 1.0, 'eval_runtime': 1.4483, 'eval_samples_per_second': 138.095, 'eval_steps_per_second': 1.381, 'epoch': 41.2}\n",
            "100% 2/2 [00:00<00:00,  3.84it/s]\n",
            "\n",
            "\u001b[A{'loss': 0.0001, 'learning_rate': 4.186046511627907e-06, 'epoch': 41.4}\n",
            "\n",
            " 92% 2070/2250 [00:35<00:02, 66.20it/s]\u001b[A***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 128\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.832884456438478e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1_score': 1.0, 'eval_roc_auc': 1.0, 'eval_runtime': 1.4486, 'eval_samples_per_second': 138.061, 'eval_steps_per_second': 1.381, 'epoch': 41.4}\n",
            "100% 2/2 [00:00<00:00,  3.79it/s]\n",
            "\n",
            " 92% 2078/2250 [00:39<00:02, 66.20it/s]\u001b[A\n",
            " 92% 2079/2250 [00:39<00:06, 25.83it/s]\u001b[A\n",
            " 92% 2080/2250 [00:40<00:06, 25.41it/s]\u001b[A\n",
            "\u001b[A{'loss': 0.0001, 'learning_rate': 3.953488372093024e-06, 'epoch': 41.6}\n",
            "\n",
            " 92% 2080/2250 [00:40<00:06, 25.41it/s]\u001b[A***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 128\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.8164349714643322e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1_score': 1.0, 'eval_roc_auc': 1.0, 'eval_runtime': 1.4481, 'eval_samples_per_second': 138.107, 'eval_steps_per_second': 1.381, 'epoch': 41.6}\n",
            "100% 2/2 [00:00<00:00,  3.77it/s]\n",
            "\n",
            "\u001b[A{'loss': 0.0001, 'learning_rate': 3.72093023255814e-06, 'epoch': 41.8}\n",
            "\n",
            " 93% 2090/2250 [00:45<00:06, 25.41it/s]\u001b[A***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 128\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.799388676066883e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1_score': 1.0, 'eval_roc_auc': 1.0, 'eval_runtime': 1.4647, 'eval_samples_per_second': 136.55, 'eval_steps_per_second': 1.366, 'epoch': 41.8}\n",
            "100% 2/2 [00:00<00:00,  3.78it/s]\n",
            "\n",
            "\u001b[A{'loss': 0.0001, 'learning_rate': 3.488372093023256e-06, 'epoch': 42.0}\n",
            "\n",
            " 93% 2100/2250 [00:50<00:05, 25.41it/s]\u001b[A***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 128\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.7841306291520596e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1_score': 1.0, 'eval_roc_auc': 1.0, 'eval_runtime': 1.4547, 'eval_samples_per_second': 137.486, 'eval_steps_per_second': 1.375, 'epoch': 42.0}\n",
            "100% 2/2 [00:00<00:00,  3.84it/s]\n",
            "\n",
            "\u001b[A{'loss': 0.0001, 'learning_rate': 3.255813953488372e-06, 'epoch': 42.2}\n",
            "\n",
            " 94% 2110/2250 [00:56<00:05, 25.41it/s]\u001b[A***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 128\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.7689326088875532e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1_score': 1.0, 'eval_roc_auc': 1.0, 'eval_runtime': 1.463, 'eval_samples_per_second': 136.703, 'eval_steps_per_second': 1.367, 'epoch': 42.2}\n",
            "100% 2/2 [00:00<00:00,  3.80it/s]\n",
            "\n",
            " 94% 2116/2250 [00:59<00:05, 25.41it/s]\u001b[A\n",
            " 94% 2117/2250 [01:00<00:11, 11.79it/s]\u001b[A\n",
            " 94% 2118/2250 [01:00<00:11, 11.64it/s]\u001b[A\n",
            "\u001b[A{'loss': 0.0001, 'learning_rate': 3.0232558139534885e-06, 'epoch': 42.4}\n",
            "\n",
            " 94% 2120/2250 [01:01<00:11, 11.64it/s]\u001b[A***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 128\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.754985871433746e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1_score': 1.0, 'eval_roc_auc': 1.0, 'eval_runtime': 1.4581, 'eval_samples_per_second': 137.165, 'eval_steps_per_second': 1.372, 'epoch': 42.4}\n",
            "100% 2/2 [00:00<00:00,  3.82it/s]\n",
            "\n",
            "\u001b[A{'loss': 0.0001, 'learning_rate': 2.7906976744186046e-06, 'epoch': 42.6}\n",
            "\n",
            " 95% 2130/2250 [01:06<00:10, 11.64it/s]\u001b[A***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 128\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.7423500796430744e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1_score': 1.0, 'eval_roc_auc': 1.0, 'eval_runtime': 1.46, 'eval_samples_per_second': 136.982, 'eval_steps_per_second': 1.37, 'epoch': 42.6}\n",
            "100% 2/2 [00:00<00:00,  3.79it/s]\n",
            "\n",
            "\u001b[A{'loss': 0.0001, 'learning_rate': 2.558139534883721e-06, 'epoch': 42.8}\n",
            "\n",
            " 95% 2140/2250 [01:11<00:09, 11.64it/s]\u001b[A***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 128\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.7316807972965762e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1_score': 1.0, 'eval_roc_auc': 1.0, 'eval_runtime': 1.4793, 'eval_samples_per_second': 135.203, 'eval_steps_per_second': 1.352, 'epoch': 42.8}\n",
            "100% 2/2 [00:00<00:00,  3.75it/s]\n",
            "\n",
            "\u001b[A{'loss': 0.0, 'learning_rate': 2.325581395348837e-06, 'epoch': 43.0}\n",
            "\n",
            " 96% 2150/2250 [01:17<00:08, 11.64it/s]\u001b[A***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 128\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.7208337996853516e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1_score': 1.0, 'eval_roc_auc': 1.0, 'eval_runtime': 1.4691, 'eval_samples_per_second': 136.141, 'eval_steps_per_second': 1.361, 'epoch': 43.0}\n",
            "100% 2/2 [00:00<00:00,  3.76it/s]\n",
            "\n",
            " 96% 2151/2250 [01:18<00:15,  6.52it/s]\u001b[A\n",
            " 96% 2152/2250 [01:19<00:15,  6.47it/s]\u001b[A\n",
            "\u001b[A{'loss': 0.0, 'learning_rate': 2.0930232558139536e-06, 'epoch': 43.2}\n",
            "\n",
            " 96% 2160/2250 [01:22<00:13,  6.47it/s]\u001b[A***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 128\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.7133241019328125e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1_score': 1.0, 'eval_roc_auc': 1.0, 'eval_runtime': 1.4768, 'eval_samples_per_second': 135.426, 'eval_steps_per_second': 1.354, 'epoch': 43.2}\n",
            "100% 2/2 [00:00<00:00,  3.74it/s]\n",
            "\n",
            "\u001b[A{'loss': 0.0001, 'learning_rate': 1.86046511627907e-06, 'epoch': 43.4}\n",
            "\n",
            " 96% 2170/2250 [01:27<00:12,  6.47it/s]\u001b[A***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 128\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.704800499486737e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1_score': 1.0, 'eval_roc_auc': 1.0, 'eval_runtime': 1.4787, 'eval_samples_per_second': 135.255, 'eval_steps_per_second': 1.353, 'epoch': 43.4}\n",
            "100% 2/2 [00:00<00:00,  3.75it/s]\n",
            "\n",
            " 97% 2172/2250 [01:29<00:12,  6.47it/s]\u001b[A\n",
            " 97% 2173/2250 [01:30<00:16,  4.72it/s]\u001b[A\n",
            " 97% 2174/2250 [01:30<00:16,  4.69it/s]\u001b[A\n",
            "\u001b[A{'loss': 0.0, 'learning_rate': 1.627906976744186e-06, 'epoch': 43.6}\n",
            "\n",
            " 97% 2180/2250 [01:32<00:14,  4.69it/s]\u001b[A***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 128\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.6981257178704254e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1_score': 1.0, 'eval_roc_auc': 1.0, 'eval_runtime': 1.4786, 'eval_samples_per_second': 135.263, 'eval_steps_per_second': 1.353, 'epoch': 43.6}\n",
            "100% 2/2 [00:00<00:00,  3.72it/s]\n",
            "\n",
            " 97% 2188/2250 [01:37<00:16,  3.84it/s]\u001b[A\n",
            "\u001b[A{'loss': 0.0001, 'learning_rate': 1.3953488372093023e-06, 'epoch': 43.8}\n",
            "\n",
            " 97% 2190/2250 [01:38<00:15,  3.84it/s]\u001b[A***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 128\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.6919868105323985e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1_score': 1.0, 'eval_roc_auc': 1.0, 'eval_runtime': 1.4964, 'eval_samples_per_second': 133.656, 'eval_steps_per_second': 1.337, 'epoch': 43.8}\n",
            "100% 2/2 [00:00<00:00,  3.64it/s]\n",
            "\n",
            " 98% 2198/2250 [01:42<00:15,  3.32it/s]\u001b[A\n",
            "\u001b[A{'loss': 0.0001, 'learning_rate': 1.1627906976744186e-06, 'epoch': 44.0}\n",
            "\n",
            " 98% 2200/2250 [01:43<00:15,  3.32it/s]\u001b[A***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 128\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.6859073841478676e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1_score': 1.0, 'eval_roc_auc': 1.0, 'eval_runtime': 1.4707, 'eval_samples_per_second': 135.993, 'eval_steps_per_second': 1.36, 'epoch': 44.0}\n",
            "100% 2/2 [00:00<00:00,  3.75it/s]\n",
            "\n",
            " 98% 2205/2250 [01:46<00:15,  2.95it/s]\u001b[A\n",
            " 98% 2210/2250 [01:48<00:13,  2.91it/s]\u001b[A\n",
            "\u001b[A{'loss': 0.0, 'learning_rate': 9.30232558139535e-07, 'epoch': 44.2}\n",
            "\n",
            " 98% 2210/2250 [01:48<00:13,  2.91it/s]\u001b[A***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 128\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.682092417671811e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1_score': 1.0, 'eval_roc_auc': 1.0, 'eval_runtime': 1.4858, 'eval_samples_per_second': 134.608, 'eval_steps_per_second': 1.346, 'epoch': 44.2}\n",
            "100% 2/2 [00:00<00:00,  3.71it/s]\n",
            "\n",
            " 98% 2214/2250 [01:51<00:14,  2.54it/s]\u001b[A\n",
            " 99% 2217/2250 [01:52<00:12,  2.55it/s]\u001b[A\n",
            " 99% 2219/2250 [01:53<00:12,  2.55it/s]\u001b[A\n",
            "\u001b[A{'loss': 0.0, 'learning_rate': 6.976744186046511e-07, 'epoch': 44.4}\n",
            "\n",
            " 99% 2220/2250 [01:54<00:11,  2.55it/s]\u001b[A***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 128\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.678934106370434e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1_score': 1.0, 'eval_roc_auc': 1.0, 'eval_runtime': 1.477, 'eval_samples_per_second': 135.409, 'eval_steps_per_second': 1.354, 'epoch': 44.4}\n",
            "100% 2/2 [00:00<00:00,  3.73it/s]\n",
            "\n",
            " 99% 2221/2250 [01:55<00:13,  2.08it/s]\u001b[A\n",
            " 99% 2222/2250 [01:56<00:13,  2.11it/s]\u001b[A\n",
            " 99% 2223/2250 [01:56<00:12,  2.16it/s]\u001b[A\n",
            " 99% 2224/2250 [01:57<00:11,  2.21it/s]\u001b[A\n",
            " 99% 2225/2250 [01:57<00:11,  2.26it/s]\u001b[A\n",
            " 99% 2226/2250 [01:57<00:10,  2.33it/s]\u001b[A\n",
            " 99% 2227/2250 [01:58<00:09,  2.39it/s]\u001b[A\n",
            " 99% 2228/2250 [01:58<00:08,  2.45it/s]\u001b[A\n",
            " 99% 2229/2250 [01:58<00:08,  2.49it/s]\u001b[A\n",
            " 99% 2230/2250 [01:59<00:07,  2.53it/s]\u001b[A\n",
            "\u001b[A{'loss': 0.0001, 'learning_rate': 4.651162790697675e-07, 'epoch': 44.6}\n",
            "\n",
            " 99% 2230/2250 [01:59<00:07,  2.53it/s]\u001b[A***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 128\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.676370604604017e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1_score': 1.0, 'eval_roc_auc': 1.0, 'eval_runtime': 1.4774, 'eval_samples_per_second': 135.374, 'eval_steps_per_second': 1.354, 'epoch': 44.6}\n",
            "100% 2/2 [00:00<00:00,  3.73it/s]\n",
            "\n",
            " 99% 2231/2250 [02:01<00:15,  1.26it/s]\u001b[A\n",
            " 99% 2232/2250 [02:01<00:12,  1.47it/s]\u001b[A\n",
            " 99% 2233/2250 [02:01<00:10,  1.68it/s]\u001b[A\n",
            " 99% 2234/2250 [02:02<00:08,  1.87it/s]\u001b[A\n",
            " 99% 2235/2250 [02:02<00:07,  2.06it/s]\u001b[A\n",
            " 99% 2236/2250 [02:03<00:06,  2.20it/s]\u001b[A\n",
            " 99% 2237/2250 [02:03<00:05,  2.31it/s]\u001b[A\n",
            " 99% 2238/2250 [02:03<00:05,  2.39it/s]\u001b[A\n",
            "100% 2239/2250 [02:04<00:04,  2.48it/s]\u001b[A\n",
            "100% 2240/2250 [02:04<00:03,  2.53it/s]\u001b[A\n",
            "\u001b[A{'loss': 0.0, 'learning_rate': 2.3255813953488374e-07, 'epoch': 44.8}\n",
            "\n",
            "100% 2240/2250 [02:04<00:03,  2.53it/s]\u001b[A***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 128\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.6751786208478734e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1_score': 1.0, 'eval_roc_auc': 1.0, 'eval_runtime': 1.4849, 'eval_samples_per_second': 134.687, 'eval_steps_per_second': 1.347, 'epoch': 44.8}\n",
            "100% 2/2 [00:00<00:00,  3.71it/s]\n",
            "\n",
            "100% 2241/2250 [02:06<00:07,  1.19it/s]\u001b[A\n",
            "100% 2242/2250 [02:06<00:05,  1.42it/s]\u001b[A\n",
            "100% 2243/2250 [02:07<00:04,  1.66it/s]\u001b[A\n",
            "100% 2244/2250 [02:07<00:03,  1.86it/s]\u001b[A\n",
            "100% 2245/2250 [02:08<00:02,  2.04it/s]\u001b[A\n",
            "100% 2246/2250 [02:08<00:01,  2.19it/s]\u001b[A\n",
            "100% 2247/2250 [02:08<00:01,  2.32it/s]\u001b[A\n",
            "100% 2248/2250 [02:09<00:00,  2.40it/s]\u001b[A\n",
            "100% 2249/2250 [02:09<00:00,  2.47it/s]\u001b[A\n",
            "100% 2250/2250 [02:09<00:00,  2.52it/s]\u001b[A\n",
            "\u001b[A{'loss': 0.0001, 'learning_rate': 0.0, 'epoch': 45.0}\n",
            "\n",
            "100% 2250/2250 [02:09<00:00,  2.52it/s]\u001b[A***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 128\n",
            "                                 \n",
            "\u001b[A{'eval_loss': 2.6744040951598436e-05, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1_score': 1.0, 'eval_roc_auc': 1.0, 'eval_runtime': 1.4923, 'eval_samples_per_second': 134.021, 'eval_steps_per_second': 1.34, 'epoch': 45.0}\n",
            "100% 2/2 [00:00<00:00,  3.75it/s]\n",
            "                                 \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/drive/MyDrive/sm_training/sagemaker_train/test_model/checkpoint-2000 (score: 3.4170403523603454e-05).\n",
            "\n",
            "\u001b[A{'train_runtime': 132.1625, 'train_samples_per_second': 272.392, 'train_steps_per_second': 17.025, 'train_loss': 5.873628478083346e-06, 'epoch': 45.0}\n",
            "\n",
            "100% 2250/2250 [02:12<00:00, 17.03it/s]\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 128\n",
            "100% 2/2 [00:00<00:00,  3.64it/s]\n",
            "****** Eval Results ******\n",
            "Saving model checkpoint to /content/drive/MyDrive/sm_training/sagemaker_train/test_model\n",
            "Configuration saved in /content/drive/MyDrive/sm_training/sagemaker_train/test_model/config.json\n",
            "Model weights saved in /content/drive/MyDrive/sm_training/sagemaker_train/test_model/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/sm_training/sagemaker_train/test_model/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/sm_training/sagemaker_train/test_model/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Model Artifact as model.tar.gz"
      ],
      "metadata": {
        "id": "-0stoYHnOwVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = '/content/drive/MyDrive/sm_training/sagemaker_train/test_model'"
      ],
      "metadata": {
        "id": "ZWXU4Dh6Ovsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "import os.path\n",
        "\n",
        "def make_tarfile(output_filename, source_dir):\n",
        "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
        "        tar.add(source_dir, arcname=os.path.basename(source_dir))"
      ],
      "metadata": {
        "id": "gHHWrovGO1tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tzun1CppPP03",
        "outputId": "1f1473a8-2738-4cf6-f7ae-026c8bc3869a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/sm_training/sagemaker_train/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "make_tarfile('wiki_attacks_model.tar.gz', model_dir)"
      ],
      "metadata": {
        "id": "fD5-HJQBPA1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference Code (in progress)\n",
        "\n",
        "Resources:\n",
        "* https://github.com/aws-samples/amazon-sagemaker-bert-pytorch/blob/master/code/deploy_ei.py\n",
        "* https://github.com/aws/sagemaker-pytorch-inference-toolkit/blob/master/src/sagemaker_pytorch_serving_container/default_pytorch_inference_handler.py\n",
        "* https://github.com/aws/sagemaker-inference-toolkit/blob/master/src/sagemaker_inference/default_inference_handler.py"
      ],
      "metadata": {
        "id": "DWJr-mZXj7JH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import dependencies\n",
        "\n",
        "Note: working directory should still be the `src` directory"
      ],
      "metadata": {
        "id": "rTJEXwvwkpLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import json\n",
        "\n",
        "# import boto3\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import precision_score, recall_score, average_precision_score, roc_auc_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
        "from transformers import AdamW, AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "\n",
        "from utils import read_object"
      ],
      "metadata": {
        "id": "kdKS75kZkrTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Model Inference Script\n",
        "\n",
        "This code is based on the SageMaker hugginface inference toolkit's handler_service.py\n",
        "\n",
        "https://github.com/aws/sagemaker-huggingface-inference-toolkit/blob/main/src/sagemaker_huggingface_inference_toolkit/handler_service.py"
      ],
      "metadata": {
        "id": "fRqaGeI5gBhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from abc import ABC\n",
        "\n",
        "from sagemaker_inference import content_types, environment, utils\n",
        "from transformers.pipelines import SUPPORTED_TASKS\n",
        "\n",
        "from mms.service import PredictionException\n",
        "from sagemaker_huggingface_inference_toolkit import decoder_encoder\n",
        "from sagemaker_huggingface_inference_toolkit.transformers_utils import (\n",
        "    _is_gpu_available,\n",
        "    get_pipeline,\n",
        "    infer_task_from_model_architecture,\n",
        ")"
      ],
      "metadata": {
        "id": "7JoUn9WXf_t1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load(self, model_dir):\n",
        "  \"\"\"\n",
        "  The Load handler is responsible for loading the Hugging Face transformer model.\n",
        "  It can be overridden to load the model from storage\n",
        "  Returns:\n",
        "  hf_pipeline (Pipeline): A Hugging Face Transformer pipeline.\n",
        "  \"\"\"\n",
        "  # gets pipeline from task tag\n",
        "  if \"HF_TASK\" in os.environ:\n",
        "    hf_pipeline = get_pipeline(task=os.environ[\"HF_TASK\"], model_dir=model_dir, device=self.device)\n",
        "  elif \"config.json\" in os.listdir(model_dir):\n",
        "    task = infer_task_from_model_architecture(f\"{model_dir}/config.json\")\n",
        "    hf_pipeline = get_pipeline(task=task, model_dir=model_dir, device=self.device)\n",
        "  else:\n",
        "    raise ValueError(\n",
        "        f\"You need to define one of the following {list(SUPPORTED_TASKS.keys())} as env 'TASK'.\", 403\n",
        "        )\n",
        "  return hf_pipeline"
      ],
      "metadata": {
        "id": "mJ0RCqNcmnJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess(self, prediction, accept):\n",
        "  \"\"\"\n",
        "  The postprocess handler is responsible for serializing the prediction result to\n",
        "  the desired accept type, can handle JSON.\n",
        "  The postprocess handler can be overridden for inference response transformation\n",
        "  Args:\n",
        "    prediction (dict): a prediction result from predict\n",
        "    accept (str): type which the output data needs to be serialized\n",
        "  Returns: output data serialized\n",
        "  \"\"\"\n",
        "  return decoder_encoder.encode(prediction, accept)"
      ],
      "metadata": {
        "id": "0WuzayhIoLMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***** Use the Code Below *****"
      ],
      "metadata": {
        "id": "v5RAWUsPXsv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import json \n",
        "import torch\n",
        "from sagemaker_huggingface_inference_toolkit import decoder_encoder\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "def model_fn(model_dir):\n",
        "    \"\"\"\n",
        "    Load the model and tokenizer for inference \n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_dir).to(device)\n",
        "    \n",
        "    model_dict = {'model':model, 'tokenizer':tokenizer}\n",
        "    \n",
        "    return model_dict \n",
        "\n",
        "\n",
        "def input_fn(request_body, request_content_type):\n",
        "    \"\"\"\n",
        "    Transform the input request to a dictionary\n",
        "    \"\"\"\n",
        "    assert request_content_type=='application/json'\n",
        "    \n",
        "    request = json.loads(request_body)\n",
        "    \n",
        "    # decoded_input_data = decoder_encoder.decode(request_body, request_content_type)\n",
        "    \n",
        "    # return decoded_input_data\n",
        "\n",
        "    return request\n",
        "\n",
        "\n",
        "def predict_fn(input_data, model):\n",
        "    \"\"\"\n",
        "    Make a prediction with the model\n",
        "    \"\"\" \n",
        "\n",
        "    text = input_data.pop('inputs')\n",
        "    parameters = input_data.pop('parameters', None)\n",
        "    \n",
        "    tokenizer = model['tokenizer']\n",
        "    model = model['model']\n",
        "\n",
        "    # Parameters may or may not be passed    \n",
        "    input_ids = tokenizer(text, truncation=True, padding='longest', return_tensors=\"pt\").input_ids\n",
        "    # output = model.generate(input_ids, **parameters) if parameters is not None else model.generate(input_ids)\n",
        "    \n",
        "    # pass inputs with all kwargs in data\n",
        "    if parameters is not None:\n",
        "      prediction = model(input_ids, **parameters)\n",
        "    else:\n",
        "      prediction = model(input_ids)\n",
        "    \n",
        "    return prediction\n",
        "    \n",
        "    # return tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "\n",
        "\n",
        "def output_fn(prediction, content_type):\n",
        "    \"\"\"\n",
        "    Return model's prediction\n",
        "    \"\"\"\n",
        "    assert content_type == 'application/json'\n",
        "    return decoder_encoder.encode(prediction, accept)"
      ],
      "metadata": {
        "id": "Ihio6uEFgT_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set variables for inference code"
      ],
      "metadata": {
        "id": "gQTMCsgtj_bP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = '/content/drive/MyDrive/sm_training/sagemaker_train/test_model'"
      ],
      "metadata": {
        "id": "WydVZ8iTj6NP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_fn(model_dir):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"================ objects in model_dir ===================\")\n",
        "    print(os.listdir(model_dir))\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "    print(\"================ model loaded ===========================\")\n",
        "    return model.to(device)"
      ],
      "metadata": {
        "id": "WiDaVxyjbqXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fn(model_dir)"
      ],
      "metadata": {
        "id": "1lMzy1QqkW3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def input_fn(request_body, request_content_type):\n",
        "    \"\"\"An input_fn that loads a pickled tensor\"\"\"\n",
        "    if request_content_type == \"application/json\":\n",
        "        data = json.loads(request_body)\n",
        "        print(\"================ input text ===============\")\n",
        "        print(data)\n",
        "        \n",
        "        if isinstance(data, str):\n",
        "            data = [data]\n",
        "        elif isinstance(data, list) and len(data) > 0 and isinstance(data[0], str):\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported input type. Input type can be a string or an non-empty list. \\\n",
        "                             I got {}\".format(data))\n",
        "                       \n",
        "        #encoded = [tokenizer.encode(x, add_special_tokens=True) for x in data]\n",
        "        #encoded = tokenizer(data, add_special_tokens=True) \n",
        "        \n",
        "        # for backward compatibility use the following way to encode \n",
        "        # https://github.com/huggingface/transformers/issues/5580\n",
        "        input_ids = [tokenizer.encode(x, add_special_tokens=True) for x in data]\n",
        "        \n",
        "        print(\"================ encoded sentences ==============\")\n",
        "        print(input_ids)\n",
        "\n",
        "        # pad shorter sentence\n",
        "        padded =  torch.zeros(len(input_ids), MAX_LEN) \n",
        "        for i, p in enumerate(input_ids):\n",
        "            padded[i, :len(p)] = torch.tensor(p)\n",
        "     \n",
        "        # create mask\n",
        "        mask = (padded != 0)\n",
        "        \n",
        "        print(\"================= padded input and attention mask ================\")\n",
        "        print(padded, '\\n', mask)\n",
        "\n",
        "        return padded.long(), mask.long()\n",
        "    raise ValueError(\"Unsupported content type: {}\".format(request_content_type))\n",
        "    \n",
        "\n",
        "def predict_fn(input_data, model):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    input_id, input_mask = input_data\n",
        "    input_id = input_id.to(device)\n",
        "    input_mask = input_mask.to(device)\n",
        "    print(\"============== encoded data =================\")\n",
        "    print(input_id, input_mask)\n",
        "    with torch.no_grad():\n",
        "        y = model(input_id, attention_mask=input_mask)[0]\n",
        "        print(\"=============== inference result =================\")\n",
        "        print(y)\n",
        "    return y"
      ],
      "metadata": {
        "id": "_5rdU4s-kUQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Renaming dict objects"
      ],
      "metadata": {
        "id": "0A6TRyK3X0Sj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result = {'epoch': 2.0,\n",
        " 'eval_f1_score': 0.7644970414201182,\n",
        " 'eval_loss': 0.14108391106128693,\n",
        " 'eval_precision': 0.7916666666666666,\n",
        " 'eval_recall': 0.7391304347826086,\n",
        " 'eval_roc_auc': 0.8576370669562777,\n",
        " 'eval_runtime': 43.9869,\n",
        " 'eval_samples_per_second': 90.936,\n",
        " 'eval_steps_per_second': 0.727}"
      ],
      "metadata": {
        "id": "s-mWJGSrX3DL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ln5LYlSX9Hy",
        "outputId": "08b67115-8fc9-44d2-b89e-52e725e19a84"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 2.0,\n",
              " 'eval_f1_score': 0.7644970414201182,\n",
              " 'eval_loss': 0.14108391106128693,\n",
              " 'eval_precision': 0.7916666666666666,\n",
              " 'eval_recall': 0.7391304347826086,\n",
              " 'eval_roc_auc': 0.8576370669562777,\n",
              " 'eval_runtime': 43.9869,\n",
              " 'eval_samples_per_second': 90.936,\n",
              " 'eval_steps_per_second': 0.727}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Export eval results as json\n",
        "# keep = ['eval_loss', 'eval_precision', 'eval_recall', 'eval_f1_score', 'eval_roc_auc']\n",
        "keep = ['eval_f1_score', 'eval_loss', 'eval_precision', 'eval_recall', 'eval_roc_auc']\n",
        "partial_dict = {k: v for k, v in eval_result.items() if k in keep}\n",
        "partial_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwtqaQcdX-En",
        "outputId": "b9224d51-b9cf-4dda-8980-0202f4aa6ae2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_f1_score': 0.7644970414201182,\n",
              " 'eval_loss': 0.14108391106128693,\n",
              " 'eval_precision': 0.7916666666666666,\n",
              " 'eval_recall': 0.7391304347826086,\n",
              " 'eval_roc_auc': 0.8576370669562777}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # inititialising dictionary\n",
        "# ini_dict = {'nikhil': 1, 'vashu' : 5, \n",
        "#             'manjeet' : 10, 'akshat' : 15}\n",
        "  \n",
        "# initializing new dict key names\n",
        "new_keys = ['F1', 'Loss', 'Precision', 'Recall', 'ROC_AUC']\n",
        "# new_names = ['Loss', 'Precision', 'Recall', 'F1', 'ROC_AUC']\n",
        "  \n",
        "# # printing initial json\n",
        "# print (\"initial 1st dictionary\", partial_dict)\n",
        "\n",
        "# print('\\n')\n",
        "  \n",
        "# changing keys of dictionary\n",
        "final_dict = dict(zip(new_keys, list(partial_dict.values())))\n",
        "  \n",
        "# printing final result\n",
        "# print (\"final dictionary\", str(final_dict))\n",
        "final_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKOiq3JjgpeJ",
        "outputId": "22b3163a-fdc0-4925-f5e5-feb145fe5895"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'F1': 0.7644970414201182,\n",
              " 'Loss': 0.14108391106128693,\n",
              " 'Precision': 0.7916666666666666,\n",
              " 'ROC_AUC': 0.8576370669562777,\n",
              " 'Recall': 0.7391304347826086}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_key_map = {'eval_loss'      : 'Loss' ,\n",
        "                'eval_precision' : 'Precision' ,\n",
        "                'eval_recall'    : 'Recall', \n",
        "                'eval_f1_score'  : 'f1',\n",
        "                'eval_roc_auc'   : 'ROC_AUC'\n",
        "                }"
      ],
      "metadata": {
        "id": "ZXDXRuVxYLCy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# target_dict = {'k1':'v1', 'k2':'v2', 'k3':'v3'}\n",
        "new_keys = ['F1','Loss','Precision', 'Recall', 'ROC_AUC']\n",
        "\n",
        "for key,n_key in zip(partial_dict.keys(), new_keys):\n",
        "    partial_dict[n_key] = partial_dict.pop(key)\n",
        "\n",
        "partial_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POjBC401a1yv",
        "outputId": "fef1a128-ea88-4fb6-a589-ab52eb1e0af2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Loss': 0.7916666666666666,\n",
              " 'Precision': 0.7391304347826086,\n",
              " 'ROC_AUC': 0.7644970414201182,\n",
              " 'Recall': 0.8576370669562777,\n",
              " 'eval_loss': 0.14108391106128693}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_dict = {names_key.get(k, k): v for k, v in partial_dict.items()}"
      ],
      "metadata": {
        "id": "yLlsYvpQeD4r"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-P8ioyLmq80",
        "outputId": "d0a4d7e7-cc32-46a5-d0e1-0a74bb75ce80"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Loss': 0.14108391106128693,\n",
              " 'Precision': 0.7916666666666666,\n",
              " 'ROC_AUC': 0.8576370669562777,\n",
              " 'Recall': 0.7391304347826086,\n",
              " 'f1': 0.7644970414201182}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading Log Files with Tensorboard"
      ],
      "metadata": {
        "id": "pvEnRjkuqjDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def parse_run_progresscsv(run_folder,\n",
        "                          fillna=False,\n",
        "                          verbose=False) -> pd.DataFrame:\n",
        "  \"\"\"Create a pandas DataFrame object from progress.csv per convention.\"\"\"\n",
        "  # Try progress.csv or log.csv from folder\n",
        "  detected_csv = None\n",
        "  for fname in ('progress.csv', 'log.csv'):\n",
        "    p = os.path.join(run_folder, fname)\n",
        "    if exists(p):\n",
        "      detected_csv = p\n",
        "      break\n",
        "\n",
        "  # maybe a direct file path is given instead of directory\n",
        "  if detected_csv is None:\n",
        "    if exists(run_folder) and not isdir(run_folder):\n",
        "      detected_csv = run_folder\n",
        "\n",
        "  if detected_csv is None:\n",
        "    raise FileNotFoundError(os.path.join(run_folder, \"*.csv\"))\n",
        "\n",
        "  # Read the detected file `p`\n",
        "  if verbose:\n",
        "    print(f\"parse_run (csv): Reading {detected_csv}\",\n",
        "          file=sys.stderr, flush=True)  # yapf: disable\n",
        "\n",
        "  with open(detected_csv, mode='r') as f:\n",
        "    df = pd.read_csv(f)\n",
        "\n",
        "  if fillna:\n",
        "    df = df.fillna(0)\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "def parse_run_tensorboard(run_folder,\n",
        "                          fillna=False,\n",
        "                          verbose=False) -> pd.DataFrame:\n",
        "  \"\"\"Create a pandas DataFrame from tensorboard eventfile or run directory.\"\"\"\n",
        "  event_file = list(\n",
        "      sorted(glob(os.path.join(run_folder, '*events.out.tfevents.*'))))\n",
        "\n",
        "  if not event_file:  # no event file detected\n",
        "    raise pd.errors.EmptyDataError(f\"No event file detected in {run_folder}\")\n",
        "  event_file = event_file[-1]  # pick the last one\n",
        "  if verbose:\n",
        "    print(f\"parse_run (tfevents) : Reading {event_file} ...\",\n",
        "          file=sys.stderr, flush=True)  # yapf: disable\n",
        "\n",
        "  from collections import defaultdict\n",
        "\n",
        "  from tensorflow.core.util import event_pb2\n",
        "  from tensorflow.python.framework.dtypes import DType\n",
        "  try:\n",
        "    # avoid DeprecationWarning on tf_record_iterator\n",
        "    from tensorflow.python._pywrap_record_io import RecordIterator\n",
        "\n",
        "    def summary_iterator(path):\n",
        "      for r in RecordIterator(path, \"\"):\n",
        "        yield event_pb2.Event.FromString(r)\n",
        "  except:\n",
        "    from tensorflow.python.summary.summary_iterator import \\\n",
        "        summary_iterator  # type: ignore"
      ],
      "metadata": {
        "id": "IkzWVm5GgN8E"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "A80q9uj9q0j0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}